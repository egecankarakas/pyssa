{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0Z4knFq7f_Z",
        "outputId": "f67b266f-199b-47e9-fdec-2cf8bdb92e1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished loading.  03:21\n",
            "cutoff 38625\n",
            "cutoff2 46902\n",
            "Creating training dataset... \n",
            "3862\n",
            "Creating validation dataset...\n",
            "Creating finaltest dataset...\n",
            "Train length: 3862\n",
            "Test length: 827\n",
            "Finaltesting length: 827\n",
            "time:  03:23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:241: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:243: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:245: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3862,)\n",
            "numpy array done.  03:23\n",
            "3862 samples in the training set.\n",
            "827 samples in the validation set.\n",
            "827 samples in the final test set.\n",
            "percentage of vulnerable samples: 8.82%\n",
            "absolute amount of vulnerable samples in test set: 83\n",
            "Starting LSTM:  03:23\n",
            "Dropout: 0.5\n",
            "Neurons: 100\n",
            "Optimizer: adam\n",
            "Epochs: 50\n",
            "Batch Size: 128\n",
            "max length: 200\n",
            "Compiled LSTM:  03:23\n",
            "Epoch 1/50\n",
            "31/31 [==============================] - 20s 514ms/step - loss: 0.8339 - f1: 0.1595\n",
            "Epoch 2/50\n",
            "31/31 [==============================] - 16s 511ms/step - loss: 0.8066 - f1: 0.1987\n",
            "Epoch 3/50\n",
            "31/31 [==============================] - 16s 514ms/step - loss: 0.7256 - f1: 0.2810\n",
            "Epoch 4/50\n",
            "31/31 [==============================] - 16s 518ms/step - loss: 0.6738 - f1: 0.3283\n",
            "Epoch 5/50\n",
            "31/31 [==============================] - 17s 557ms/step - loss: 0.6231 - f1: 0.3948\n",
            "Epoch 6/50\n",
            "31/31 [==============================] - 16s 513ms/step - loss: 0.6150 - f1: 0.3739\n",
            "Epoch 7/50\n",
            "31/31 [==============================] - 16s 518ms/step - loss: 0.6207 - f1: 0.3728\n",
            "Epoch 8/50\n",
            "31/31 [==============================] - 16s 523ms/step - loss: 0.5799 - f1: 0.4298\n",
            "Epoch 9/50\n",
            "31/31 [==============================] - 16s 515ms/step - loss: 0.5563 - f1: 0.4306\n",
            "Epoch 10/50\n",
            "31/31 [==============================] - 16s 519ms/step - loss: 0.5382 - f1: 0.4570\n",
            "Epoch 11/50\n",
            "31/31 [==============================] - 16s 529ms/step - loss: 0.5080 - f1: 0.5054\n",
            "Epoch 12/50\n",
            "31/31 [==============================] - 17s 534ms/step - loss: 0.5265 - f1: 0.4727\n",
            "Epoch 13/50\n",
            "31/31 [==============================] - 16s 525ms/step - loss: 0.5789 - f1: 0.4130\n",
            "Epoch 14/50\n",
            "31/31 [==============================] - 16s 520ms/step - loss: 0.6686 - f1: 0.3315\n",
            "Epoch 15/50\n",
            "31/31 [==============================] - 16s 525ms/step - loss: 0.5256 - f1: 0.4865\n",
            "Epoch 16/50\n",
            "31/31 [==============================] - 16s 519ms/step - loss: 0.4732 - f1: 0.5392\n",
            "Epoch 17/50\n",
            "31/31 [==============================] - 16s 517ms/step - loss: 0.4653 - f1: 0.5242\n",
            "Epoch 18/50\n",
            "31/31 [==============================] - 16s 508ms/step - loss: 0.4618 - f1: 0.5466\n",
            "Epoch 19/50\n",
            "31/31 [==============================] - 16s 523ms/step - loss: 0.4445 - f1: 0.5709\n",
            "Epoch 20/50\n",
            "31/31 [==============================] - 20s 634ms/step - loss: 0.4519 - f1: 0.5648\n",
            "Epoch 21/50\n",
            "31/31 [==============================] - 16s 509ms/step - loss: 0.4188 - f1: 0.6029\n",
            "Epoch 22/50\n",
            "31/31 [==============================] - 16s 514ms/step - loss: 0.4161 - f1: 0.6054\n",
            "Epoch 23/50\n",
            "31/31 [==============================] - 16s 510ms/step - loss: 0.4144 - f1: 0.5865\n",
            "Epoch 24/50\n",
            "31/31 [==============================] - 16s 512ms/step - loss: 0.4576 - f1: 0.5527\n",
            "Epoch 25/50\n",
            "31/31 [==============================] - 16s 515ms/step - loss: 0.4046 - f1: 0.6201\n",
            "Epoch 26/50\n",
            "31/31 [==============================] - 16s 519ms/step - loss: 0.3983 - f1: 0.6108\n",
            "Epoch 27/50\n",
            "31/31 [==============================] - 16s 507ms/step - loss: 0.4136 - f1: 0.5909\n",
            "Epoch 28/50\n",
            "31/31 [==============================] - 16s 506ms/step - loss: 0.4146 - f1: 0.5878\n",
            "Epoch 29/50\n",
            "31/31 [==============================] - 15s 500ms/step - loss: 0.4012 - f1: 0.5965\n",
            "Epoch 30/50\n",
            "31/31 [==============================] - 16s 506ms/step - loss: 0.4100 - f1: 0.5978\n",
            "Epoch 31/50\n",
            "31/31 [==============================] - 16s 501ms/step - loss: 0.3789 - f1: 0.6344\n",
            "Epoch 32/50\n",
            "31/31 [==============================] - 16s 501ms/step - loss: 0.3641 - f1: 0.6510\n",
            "Epoch 33/50\n",
            "31/31 [==============================] - 16s 501ms/step - loss: 0.3633 - f1: 0.6217\n",
            "Epoch 34/50\n",
            "31/31 [==============================] - 15s 497ms/step - loss: 0.3578 - f1: 0.6256\n",
            "Epoch 35/50\n",
            "31/31 [==============================] - 16s 508ms/step - loss: 0.3883 - f1: 0.6231\n",
            "Epoch 36/50\n",
            "31/31 [==============================] - 15s 499ms/step - loss: 0.3645 - f1: 0.6414\n",
            "Epoch 37/50\n",
            "31/31 [==============================] - 15s 499ms/step - loss: 0.3678 - f1: 0.6361\n",
            "Epoch 38/50\n",
            "31/31 [==============================] - 16s 503ms/step - loss: 0.3651 - f1: 0.6322\n",
            "Epoch 39/50\n",
            "31/31 [==============================] - 16s 500ms/step - loss: 0.3543 - f1: 0.6460\n",
            "Epoch 40/50\n",
            "31/31 [==============================] - 16s 501ms/step - loss: 0.3348 - f1: 0.6874\n",
            "Epoch 41/50\n",
            "31/31 [==============================] - 16s 505ms/step - loss: 0.3162 - f1: 0.7094\n",
            "Epoch 42/50\n",
            "31/31 [==============================] - 16s 501ms/step - loss: 0.3377 - f1: 0.6483\n",
            "Epoch 43/50\n",
            "31/31 [==============================] - 15s 499ms/step - loss: 0.3160 - f1: 0.6889\n",
            "Epoch 44/50\n",
            "31/31 [==============================] - 15s 498ms/step - loss: 0.3197 - f1: 0.6736\n",
            "Epoch 45/50\n",
            "31/31 [==============================] - 16s 514ms/step - loss: 0.3184 - f1: 0.6917\n",
            "Epoch 46/50\n",
            "31/31 [==============================] - 16s 500ms/step - loss: 0.3171 - f1: 0.6940\n",
            "Epoch 47/50\n",
            "31/31 [==============================] - 15s 499ms/step - loss: 0.3196 - f1: 0.6641\n",
            "Epoch 48/50\n",
            "31/31 [==============================] - 15s 500ms/step - loss: 0.3103 - f1: 0.6815\n",
            "Epoch 49/50\n",
            "31/31 [==============================] - 16s 507ms/step - loss: 0.3200 - f1: 0.6855\n",
            "Epoch 50/50\n",
            "31/31 [==============================] - 16s 501ms/step - loss: 0.2998 - f1: 0.7113\n",
            "Now predicting on train set (0.5 dropout)\n",
            "Accuracy: 0.9606421543241843\n",
            "Precision: 0.9609756097560975\n",
            "Recall: 0.5777126099706745\n",
            "F1 score: 0.721612\n",
            "\n",
            "\n",
            "Now predicting on test set (0.5 dropout)\n",
            "Accuracy: 0.9359129383313181\n",
            "Precision: 0.8260869565217391\n",
            "Recall: 0.4578313253012048\n",
            "F1 score: 0.589147\n",
            "\n",
            "\n",
            "Now predicting on finaltest set (0.5 dropout)\n",
            "Accuracy: 0.939540507859734\n",
            "Precision: 0.9090909090909091\n",
            "Recall: 0.38961038961038963\n",
            "F1 score: 0.545455\n",
            "\n",
            "\n",
            "saving LSTM model .  03:36\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import utils\n",
        "import sys\n",
        "import os.path\n",
        "import json\n",
        "from datetime import datetime\n",
        "import random\n",
        "import pickle\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.preprocessing import sequence\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils import class_weight\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.models import FastText, KeyedVectors\n",
        "#from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, TFAutoModel\n",
        "import torch\n",
        "#from bert_embedding import BertEmbedding\n",
        "\n",
        "\n",
        "\n",
        "mincount = 10\n",
        "iterationen = 100\n",
        "s = 200 \n",
        "w = \"withString\" \n",
        "\n",
        "\n",
        "w2v = \"word2vec_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s)\n",
        "w2vmodel = \"drive/MyDrive/SSS/Word2v-embbeding/\" + w2v + \".model\"\n",
        "# w2vmodel = \"Word2v-embbeding/\" + w2v + \".model\"\n",
        "\n",
        "#f2v = \"fasttext_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s)\n",
        "#f2vmodel = \"FastText-embbeding/\" + f2v + \".model\"\n",
        "\n",
        "#bert_embedding = BertEmbedding()\n",
        "\n",
        "if not (os.path.isfile(w2vmodel)):\n",
        "  print(\"word2vec model is still being created...\")\n",
        "  print(\"w2vmodel\")\n",
        "  sys.exit()\n",
        "\n",
        "\n",
        "model = Word2Vec.load(w2vmodel)\n",
        "word_vectors = model.wv\n",
        "\n",
        "\n",
        "#model = FastText.load(f2vmodel)\n",
        "#word_vectors = f2v_model.wv\n",
        "\n",
        "\n",
        "with open('drive/MyDrive/SSS/data/PyCommitsWithDiffs' , 'r') as infile:\n",
        "# with open('data/PyCommitsWithDiffs' , 'r') as infile:\n",
        "  data = json.load(infile)\n",
        "  \n",
        "now = datetime.now() # current date and time\n",
        "nowformat = now.strftime(\"%H:%M\")\n",
        "print(\"finished loading. \", nowformat)\n",
        "\n",
        "\n",
        "progress = 0\n",
        "count = 0\n",
        "\n",
        "\n",
        "restriction = [20000,5,6,10] \n",
        "step = 5 \n",
        "fulllength = 200 \n",
        "\n",
        "allblocks = []\n",
        "\n",
        "for r in data:\n",
        "  progress = progress + 1\n",
        "  \n",
        "  for c in data[r]:\n",
        "    \n",
        "    if \"files\" in data[r][c]:                      \n",
        "      \n",
        "      for f in data[r][c][\"files\"]:\n",
        "        \n",
        "        \n",
        "        if not \"source\" in data[r][c][\"files\"][f]:\n",
        " \n",
        "          continue\n",
        "        \n",
        "        if \"source\" in data[r][c][\"files\"][f]:\n",
        "          sourcecode = data[r][c][\"files\"][f][\"source\"]                          \n",
        "          \n",
        "          allbadparts = []\n",
        "          \n",
        "          for change in data[r][c][\"files\"][f][\"changes\"]:\n",
        "                             \n",
        "                badparts = change[\"badparts\"]\n",
        "                count = count + len(badparts)\n",
        "                                \n",
        "                for bad in badparts:\n",
        "    \n",
        "                  pos = utils.findposition(bad, sourcecode)\n",
        "                  if not -1 in pos:\n",
        "                      allbadparts.append(bad)\n",
        "                      \n",
        "                      \n",
        "          if(len(allbadparts) > 0):\n",
        "              positions = utils.findpositions(allbadparts, sourcecode)\n",
        "\n",
        "              blocks = utils.getblocks(sourcecode, positions, step, fulllength)\n",
        "              \n",
        "              for b in blocks:\n",
        "\n",
        "                  allblocks.append(b)\n",
        "\n",
        "\n",
        "keys = []\n",
        "\n",
        "for i in range(len(allblocks)):\n",
        "  keys.append(i)\n",
        "random.shuffle(keys)\n",
        "\n",
        "\n",
        "cutoff = round(0.7 * len(keys)) \n",
        "cutoff2 = round(0.85 * len(keys)) \n",
        "\n",
        "keystrain = keys[:cutoff]\n",
        "keystrain = keystrain[:int(len(keystrain)/10)]\n",
        "keystest = keys[cutoff:cutoff2]\n",
        "keystest = keystest[:int(len(keystest)/10)]\n",
        "keysfinaltest = keys[cutoff2:]\n",
        "keysfinaltest = keysfinaltest[:int(len(keysfinaltest)/10)]\n",
        "\n",
        "print(\"cutoff \" + str(cutoff))\n",
        "print(\"cutoff2 \" + str(cutoff2))\n",
        "\n",
        "\n",
        "# with open('drive/MyDrive/SSS/data/dataset_keystrain', 'wb') as fp:\n",
        "#   pickle.dump(keystrain, fp)\n",
        "# with open('drive/MyDrive/SSS/data/dataset_keystest', 'wb') as fp:\n",
        "#   pickle.dump(keystest, fp)\n",
        "# with open('drive/MyDrive/SSS/data/dataset_keysfinaltest', 'wb') as fp:\n",
        "#   pickle.dump(keysfinaltest, fp)\n",
        "\n",
        "TrainX = []\n",
        "TrainY = []\n",
        "ValidateX = []\n",
        "ValidateY = []\n",
        "FinaltestX = []\n",
        "FinaltestY = []\n",
        "\n",
        "\n",
        "print(\"Creating training dataset... \")\n",
        "print(len(keystrain))\n",
        "\n",
        "for k in keystrain:\n",
        "  block = allblocks[k]    \n",
        "  code = block[0]\n",
        "  token = utils.getTokens(code) \n",
        "  vectorlist = []\n",
        "  for t in token: \n",
        "    if t != \" \" and word_vectors.key_to_index.get(t):\n",
        "      vector = model.wv.get_vector(t)\n",
        "      vectorlist.append(vector.tolist()) \n",
        "  TrainX.append(vectorlist) \n",
        "  TrainY.append(block[1]) \n",
        "\n",
        "print(\"Creating validation dataset...\")\n",
        "for k in keystest:\n",
        "  block = allblocks[k]\n",
        "  code = block[0]\n",
        "  token = utils.getTokens(code) \n",
        "  vectorlist = []\n",
        "  for t in token: \n",
        "    if t != \" \" and word_vectors.key_to_index.get(t):\n",
        "      vector = model.wv.get_vector(t)\n",
        "      vectorlist.append(vector.tolist()) \n",
        "  ValidateX.append(vectorlist) \n",
        "  ValidateY.append(block[1]) \n",
        "\n",
        "print(\"Creating finaltest dataset...\")\n",
        "for k in keysfinaltest:\n",
        "  block = allblocks[k]  \n",
        "  code = block[0]\n",
        "  token = utils.getTokens(code) \n",
        "  vectorlist = []\n",
        "  for t in token: \n",
        "    if t != \" \" and word_vectors.key_to_index.get(t):\n",
        "      vector = model.wv.get_vector(t)\n",
        "      vectorlist.append(vector.tolist()) \n",
        "  FinaltestX.append(vectorlist) \n",
        "  FinaltestY.append(block[1]) \n",
        "\n",
        "#for bert\n",
        "#print(\"Creating training dataset... \")\n",
        "#for k in keystrain:\n",
        "  #block = allblocks[k]    \n",
        "  #code = block[0]\n",
        "  #sentences = code.split('\\n')\n",
        "  #result = bert_embedding(sentences)\n",
        "  #for i in range(len(result)):\n",
        "    #token = result[i]\n",
        "    #tokens = token[1]\n",
        "  #TrainX.append(tokens) \n",
        "  #TrainY.append(block[1]) \n",
        "\n",
        "#print(\"Creating validation dataset...\")\n",
        "#for k in keystest:\n",
        "  #block = allblocks[k]\n",
        "  #code = block[0]\n",
        "  #sentences = code.split('\\n')\n",
        "  #result = bert_embedding(sentences)\n",
        "  #for i in range(len(result)):\n",
        "   #token = result[i]\n",
        "    #tokens = token[1]\n",
        "  #ValidateX.append(tokens) \n",
        "  #ValidateY.append(block[1]) \n",
        "\n",
        "#print(\"Creating finaltest dataset...\")\n",
        "#for k in keysfinaltest:\n",
        "  #block = allblocks[k]  \n",
        "  #code = block[0]\n",
        "  #sentences = code.split('\\n')\n",
        "  #result = bert_embedding(sentences)\n",
        "  #for i in range(len(result)):\n",
        "    #token = result[i]\n",
        "    #tokens = token[1]\n",
        "  #FinaltestX.append(tokens)  \n",
        "  #FinaltestY.append(block[1]) \n",
        "\n",
        "\n",
        "print(\"Train length: \" + str(len(TrainX)))\n",
        "print(\"Test length: \" + str(len(ValidateX)))\n",
        "print(\"Finaltesting length: \" + str(len(FinaltestX)))\n",
        "now = datetime.now() # current date and time\n",
        "nowformat = now.strftime(\"%H:%M\")\n",
        "print(\"time: \", nowformat)\n",
        "\n",
        "\n",
        "\n",
        "X_train =  numpy.array(TrainX)\n",
        "y_train =  numpy.array(TrainY)\n",
        "X_test =  numpy.array(ValidateX)\n",
        "y_test =  numpy.array(ValidateY)\n",
        "X_finaltest =  numpy.array(FinaltestX)\n",
        "y_finaltest =  numpy.array(FinaltestY)\n",
        "print(X_train.shape)\n",
        "for i in range(len(y_train)):\n",
        "  if y_train[i] == 0:\n",
        "    y_train[i] = 1\n",
        "  else:\n",
        "    y_train[i] = 0\n",
        "    \n",
        "for i in range(len(y_test)):\n",
        "  if y_test[i] == 0:\n",
        "    y_test[i] = 1\n",
        "  else:\n",
        "    y_test[i] = 0\n",
        "    \n",
        "for i in range(len(y_finaltest)):\n",
        "  if y_finaltest[i] == 0:\n",
        "    y_finaltest[i] = 1\n",
        "  else:\n",
        "    y_finaltest[i] = 0\n",
        "\n",
        "\n",
        "now = datetime.now() \n",
        "nowformat = now.strftime(\"%H:%M\")\n",
        "print(\"numpy array done. \", nowformat)\n",
        "\n",
        "print(str(len(X_train)) + \" samples in the training set.\")      \n",
        "print(str(len(X_test)) + \" samples in the validation set.\") \n",
        "print(str(len(X_finaltest)) + \" samples in the final test set.\")\n",
        "  \n",
        "csum = 0\n",
        "for a in y_train:\n",
        "  csum = csum+a\n",
        "print(\"percentage of vulnerable samples: \"  + str(int((csum / len(X_train)) * 10000)/100) + \"%\")\n",
        "  \n",
        "testvul = 0\n",
        "for y in y_test:\n",
        "  if y == 1:\n",
        "    testvul = testvul+1\n",
        "print(\"absolute amount of vulnerable samples in test set: \" + str(testvul))\n",
        "\n",
        "max_length = fulllength \n",
        "  \n",
        "\n",
        "#hyperparameters for the LSTM model\n",
        "\n",
        "dropout = 0.5\n",
        "neurons = 100\n",
        "optimizer = \"adam\"\n",
        "epochs = 50\n",
        "batchsize = 128\n",
        "\n",
        "now = datetime.now() # current date and time\n",
        "nowformat = now.strftime(\"%H:%M\")\n",
        "print(\"Starting LSTM: \", nowformat)\n",
        "\n",
        "\n",
        "print(\"Dropout: \" + str(dropout))\n",
        "print(\"Neurons: \" + str(neurons))\n",
        "print(\"Optimizer: \" + optimizer)\n",
        "print(\"Epochs: \" + str(epochs))\n",
        "print(\"Batch Size: \" + str(batchsize))\n",
        "print(\"max length: \" + str(max_length))\n",
        "\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=100, dtype='float32')\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=100, dtype='float32')\n",
        "X_finaltest = sequence.pad_sequences(X_finaltest, maxlen=100, dtype='float32')\n",
        "\n",
        "#creating the model  \n",
        "model = Sequential()\n",
        "model.add(LSTM(neurons, dropout = dropout, recurrent_dropout = dropout)) #around 50 seems good\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss=utils.f1_loss, optimizer='adam', metrics=[utils.f1])\n",
        "\n",
        "now = datetime.now() \n",
        "nowformat = now.strftime(\"%H:%M\")\n",
        "print(\"Compiled LSTM: \", nowformat)\n",
        "\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced', classes=numpy.unique(y_train), y=y_train)\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "y_train = tf.cast(y_train, tf.float32)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batchsize, class_weight=class_weights)\n",
        "\n",
        "\n",
        "\n",
        "for dataset in [\"train\",\"test\",\"finaltest\"]:\n",
        "    print(\"Now predicting on \" + dataset + \" set (\" + str(dropout) + \" dropout)\")\n",
        "    \n",
        "    if dataset == \"train\":\n",
        "      yhat_classes = (model.predict(X_train) > 0.5).astype(\"float32\")\n",
        "      accuracy = accuracy_score(y_train, yhat_classes)\n",
        "      precision = precision_score(y_train, yhat_classes)\n",
        "      recall = recall_score(y_train, yhat_classes)\n",
        "      F1Score = f1_score(y_train, yhat_classes)\n",
        "      \n",
        "    if dataset == \"test\":\n",
        "      yhat_classes = (model.predict(X_test) > 0.5).astype(\"float32\")\n",
        "      accuracy = accuracy_score(y_test, yhat_classes)\n",
        "      precision = precision_score(y_test, yhat_classes)\n",
        "      recall = recall_score(y_test, yhat_classes)\n",
        "      F1Score = f1_score(y_test, yhat_classes)\n",
        "      \n",
        "      \n",
        "    if dataset == \"finaltest\":\n",
        "      yhat_classes = (model.predict(X_finaltest) > 0.5).astype(\"float32\")\n",
        "      accuracy = accuracy_score(y_finaltest, yhat_classes)\n",
        "      precision = precision_score(y_finaltest, yhat_classes)\n",
        "      recall = recall_score(y_finaltest, yhat_classes)\n",
        "      F1Score = f1_score(y_finaltest, yhat_classes)\n",
        "      \n",
        "    print(\"Accuracy: \" + str(accuracy))\n",
        "    print(\"Precision: \" + str(precision))\n",
        "    print(\"Recall: \" + str(recall))\n",
        "    print('F1 score: %f' % F1Score)\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "now = datetime.now() \n",
        "nowformat = now.strftime(\"%H:%M\")\n",
        "print(\"saving LSTM model \"  \". \", nowformat)\n",
        "model.save('model/LSTM_model.h5')  \n",
        "print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install tqdm boto3 requests regex sentencepiece sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifEnZbuLDEYS",
        "outputId": "e8c5e061-402d-44eb-c123-eab96a48c871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.20.23-py3-none-any.whl (131 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "Collecting botocore<1.24.0,>=1.23.23\n",
            "  Downloading botocore-1.23.23-py3-none-any.whl (8.4 MB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.23->boto3) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.23->boto3) (1.15.0)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, sacremoses, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.20.23 botocore-1.23.23 jmespath-0.10.0 s3transfer-0.5.0 sacremoses-0.0.46 sentencepiece-0.1.96 urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim==4.1.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jxr4pqXMDF00",
        "outputId": "539ca28d-2407-4e35-8975-94bbf9ca058b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==4.1.2 in /usr/local/lib/python3.7/dist-packages (4.1.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "gensim.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6Lb2thAfFc9V",
        "outputId": "5216ff53-0e9d-4ba0-856c-ec255cdad994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'4.1.2'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LgqOn63rUb5d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}