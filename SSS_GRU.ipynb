{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSS_GRU.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0Z4knFq7f_Z",
        "outputId": "969af0ef-3f80-49a6-ca04-d6d50092169d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished loading.  21:38\n",
            "cutoff 91842\n",
            "cutoff2 111523\n",
            "Creating training dataset... \n",
            "5000\n",
            "Creating validation dataset...\n",
            "Creating finaltest dataset...\n",
            "Train length: 5000\n",
            "Test length: 5000\n",
            "Finaltesting length: 5000\n",
            "time:  21:42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:242: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:244: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:246: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000,)\n",
            "numpy array done.  21:42\n",
            "5000 samples in the training set.\n",
            "5000 samples in the validation set.\n",
            "5000 samples in the final test set.\n",
            "percentage of vulnerable samples: 11.62%\n",
            "absolute amount of vulnerable samples in test set: 576\n",
            "Starting GRU:  21:42\n",
            "Dropout: 0.5\n",
            "Neurons: 100\n",
            "Optimizer: adam\n",
            "Epochs: 50\n",
            "Batch Size: 128\n",
            "max length: 200\n",
            "Compiled LSTM:  21:42\n",
            "Epoch 1/50\n",
            "40/40 [==============================] - 23s 486ms/step - loss: 0.7916 - f1: 0.1962\n",
            "Epoch 2/50\n",
            "40/40 [==============================] - 20s 488ms/step - loss: 0.7863 - f1: 0.2085\n",
            "Epoch 3/50\n",
            "40/40 [==============================] - 22s 563ms/step - loss: 0.7773 - f1: 0.2219\n",
            "Epoch 4/50\n",
            "40/40 [==============================] - 20s 493ms/step - loss: 0.7695 - f1: 0.2446\n",
            "Epoch 5/50\n",
            "40/40 [==============================] - 20s 490ms/step - loss: 0.7516 - f1: 0.2680\n",
            "Epoch 6/50\n",
            "40/40 [==============================] - 19s 483ms/step - loss: 0.7159 - f1: 0.3034\n",
            "Epoch 7/50\n",
            "40/40 [==============================] - 19s 480ms/step - loss: 0.6731 - f1: 0.3216\n",
            "Epoch 8/50\n",
            "40/40 [==============================] - 19s 480ms/step - loss: 0.6543 - f1: 0.3406\n",
            "Epoch 9/50\n",
            "40/40 [==============================] - 20s 498ms/step - loss: 0.6252 - f1: 0.3850\n",
            "Epoch 10/50\n",
            "40/40 [==============================] - 20s 492ms/step - loss: 0.5972 - f1: 0.3937\n",
            "Epoch 11/50\n",
            "40/40 [==============================] - 19s 486ms/step - loss: 0.5737 - f1: 0.4141\n",
            "Epoch 12/50\n",
            "40/40 [==============================] - 20s 494ms/step - loss: 0.5771 - f1: 0.4201\n",
            "Epoch 13/50\n",
            "40/40 [==============================] - 20s 489ms/step - loss: 0.5422 - f1: 0.4532\n",
            "Epoch 14/50\n",
            "40/40 [==============================] - 19s 484ms/step - loss: 0.5212 - f1: 0.4808\n",
            "Epoch 15/50\n",
            "40/40 [==============================] - 20s 499ms/step - loss: 0.5108 - f1: 0.4917\n",
            "Epoch 16/50\n",
            "40/40 [==============================] - 20s 502ms/step - loss: 0.5287 - f1: 0.4838\n",
            "Epoch 17/50\n",
            "40/40 [==============================] - 19s 485ms/step - loss: 0.5207 - f1: 0.4923\n",
            "Epoch 18/50\n",
            "40/40 [==============================] - 19s 487ms/step - loss: 0.5528 - f1: 0.4468\n",
            "Epoch 19/50\n",
            "40/40 [==============================] - 20s 494ms/step - loss: 0.5254 - f1: 0.4734\n",
            "Epoch 20/50\n",
            "40/40 [==============================] - 20s 489ms/step - loss: 0.5052 - f1: 0.5259\n",
            "Epoch 21/50\n",
            "40/40 [==============================] - 20s 490ms/step - loss: 0.4884 - f1: 0.5278\n",
            "Epoch 22/50\n",
            "40/40 [==============================] - 20s 494ms/step - loss: 0.5126 - f1: 0.4824\n",
            "Epoch 23/50\n",
            "40/40 [==============================] - 20s 496ms/step - loss: 0.4759 - f1: 0.5171\n",
            "Epoch 24/50\n",
            "40/40 [==============================] - 20s 501ms/step - loss: 0.4645 - f1: 0.5318\n",
            "Epoch 25/50\n",
            "40/40 [==============================] - 21s 514ms/step - loss: 0.4537 - f1: 0.5548\n",
            "Epoch 26/50\n",
            "40/40 [==============================] - 20s 492ms/step - loss: 0.4447 - f1: 0.5552\n",
            "Epoch 27/50\n",
            "40/40 [==============================] - 20s 497ms/step - loss: 0.4228 - f1: 0.5743\n",
            "Epoch 28/50\n",
            "40/40 [==============================] - 20s 498ms/step - loss: 0.4077 - f1: 0.6032\n",
            "Epoch 29/50\n",
            "40/40 [==============================] - 22s 555ms/step - loss: 0.4250 - f1: 0.5673\n",
            "Epoch 30/50\n",
            "40/40 [==============================] - 20s 493ms/step - loss: 0.4016 - f1: 0.5911\n",
            "Epoch 31/50\n",
            "40/40 [==============================] - 20s 496ms/step - loss: 0.4163 - f1: 0.5708\n",
            "Epoch 32/50\n",
            "40/40 [==============================] - 20s 491ms/step - loss: 0.4205 - f1: 0.5872\n",
            "Epoch 33/50\n",
            "40/40 [==============================] - 20s 495ms/step - loss: 0.3934 - f1: 0.5968\n",
            "Epoch 34/50\n",
            "40/40 [==============================] - 20s 490ms/step - loss: 0.4016 - f1: 0.5872\n",
            "Epoch 35/50\n",
            "40/40 [==============================] - 20s 494ms/step - loss: 0.3849 - f1: 0.6051\n",
            "Epoch 36/50\n",
            "40/40 [==============================] - 20s 492ms/step - loss: 0.3812 - f1: 0.6102\n",
            "Epoch 37/50\n",
            "40/40 [==============================] - 20s 492ms/step - loss: 0.3801 - f1: 0.6272\n",
            "Epoch 38/50\n",
            "40/40 [==============================] - 19s 487ms/step - loss: 0.3722 - f1: 0.6443\n",
            "Epoch 39/50\n",
            "40/40 [==============================] - 20s 491ms/step - loss: 0.3725 - f1: 0.6183\n",
            "Epoch 40/50\n",
            "40/40 [==============================] - 20s 492ms/step - loss: 0.3711 - f1: 0.6253\n",
            "Epoch 41/50\n",
            "40/40 [==============================] - 20s 486ms/step - loss: 0.3757 - f1: 0.6166\n",
            "Epoch 42/50\n",
            "40/40 [==============================] - 20s 488ms/step - loss: 0.3814 - f1: 0.6312\n",
            "Epoch 43/50\n",
            "40/40 [==============================] - 19s 486ms/step - loss: 0.3766 - f1: 0.6326\n",
            "Epoch 44/50\n",
            "40/40 [==============================] - 19s 482ms/step - loss: 0.3720 - f1: 0.6184\n",
            "Epoch 45/50\n",
            "40/40 [==============================] - 19s 484ms/step - loss: 0.3571 - f1: 0.6589\n",
            "Epoch 46/50\n",
            "40/40 [==============================] - 20s 488ms/step - loss: 0.3443 - f1: 0.6367\n",
            "Epoch 47/50\n",
            "40/40 [==============================] - 20s 492ms/step - loss: 0.3695 - f1: 0.6241\n",
            "Epoch 48/50\n",
            "40/40 [==============================] - 19s 486ms/step - loss: 0.3474 - f1: 0.6713\n",
            "Epoch 49/50\n",
            "40/40 [==============================] - 21s 513ms/step - loss: 0.3343 - f1: 0.6836\n",
            "Epoch 50/50\n",
            "40/40 [==============================] - 21s 538ms/step - loss: 0.3469 - f1: 0.6458\n",
            "Now predicting on train set (0.5 dropout)\n",
            "Accuracy: 0.9452\n",
            "Precision: 0.7588532883642496\n",
            "Recall: 0.774526678141136\n",
            "F1 score: 0.766610\n",
            "\n",
            "\n",
            "Now predicting on test set (0.5 dropout)\n",
            "Accuracy: 0.9038\n",
            "Precision: 0.5826086956521739\n",
            "Recall: 0.5815972222222222\n",
            "F1 score: 0.582103\n",
            "\n",
            "\n",
            "Now predicting on finaltest set (0.5 dropout)\n",
            "Accuracy: 0.905\n",
            "Precision: 0.5849731663685152\n",
            "Recall: 0.5736842105263158\n",
            "F1 score: 0.579274\n",
            "\n",
            "\n",
            "saving GRU model .  22:00\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import utils\n",
        "import sys\n",
        "import os.path\n",
        "import json\n",
        "from datetime import datetime\n",
        "import random\n",
        "import pickle\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, ConvLSTM2D, GRU\n",
        "from keras.preprocessing import sequence\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils import class_weight\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.models import FastText, KeyedVectors\n",
        "#from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, TFAutoModel\n",
        "import torch\n",
        "#from bert_embedding import BertEmbedding\n",
        "\n",
        "mincount = 10\n",
        "iterationen = 100\n",
        "s = 200 \n",
        "w = \"withString\" \n",
        "data_limit=5000\n",
        "vulnerability_type='path_disclosure'\n",
        "\n",
        "\n",
        "w2v = \"word2vec_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s)\n",
        "w2vmodel = \"drive/MyDrive/SSS/Word2v-embbeding/\" + w2v + \".model\"\n",
        "# w2vmodel = \"Word2v-embbeding/\" + w2v + \".model\"\n",
        "\n",
        "#f2v = \"fasttext_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s)\n",
        "#f2vmodel = \"FastText-embbeding/\" + f2v + \".model\"\n",
        "\n",
        "#bert_embedding = BertEmbedding()\n",
        "\n",
        "if not (os.path.isfile(w2vmodel)):\n",
        "  print(\"word2vec model is still being created...\")\n",
        "  print(\"w2vmodel\")\n",
        "  sys.exit()\n",
        "\n",
        "\n",
        "model = Word2Vec.load(w2vmodel)\n",
        "word_vectors = model.wv\n",
        "\n",
        "\n",
        "#model = FastText.load(f2vmodel)\n",
        "#word_vectors = f2v_model.wv\n",
        "\n",
        "\n",
        "with open(f'drive/MyDrive/SSS/data/PyCommitsWithDiffs_{vulnerability_type}.json' , 'r') as infile:\n",
        "# with open('data/PyCommitsWithDiffs' , 'r') as infile:\n",
        "  data = json.load(infile)\n",
        "  \n",
        "now = datetime.now() # current date and time\n",
        "nowformat = now.strftime(\"%H:%M\")\n",
        "print(\"finished loading. \", nowformat)\n",
        "\n",
        "\n",
        "progress = 0\n",
        "count = 0\n",
        "\n",
        "\n",
        "restriction = [20000,5,6,10] \n",
        "step = 5 \n",
        "fulllength = 200 \n",
        "\n",
        "allblocks = []\n",
        "\n",
        "data={k: data[k] for k in list(data)[:data_limit]}\n",
        "for r in data:\n",
        "  progress = progress + 1\n",
        "  \n",
        "  for c in data[r]:\n",
        "    \n",
        "    if \"files\" in data[r][c]:                      \n",
        "      \n",
        "      for f in data[r][c][\"files\"]:\n",
        "        \n",
        "        \n",
        "        if not \"source\" in data[r][c][\"files\"][f]:\n",
        " \n",
        "          continue\n",
        "        \n",
        "        if \"source\" in data[r][c][\"files\"][f]:\n",
        "          sourcecode = data[r][c][\"files\"][f][\"source\"]                          \n",
        "          \n",
        "          allbadparts = []\n",
        "          \n",
        "          for change in data[r][c][\"files\"][f][\"changes\"]:\n",
        "                             \n",
        "                badparts = change[\"badparts\"]\n",
        "                count = count + len(badparts)\n",
        "                                \n",
        "                for bad in badparts:\n",
        "    \n",
        "                  pos = utils.findposition(bad, sourcecode)\n",
        "                  if not -1 in pos:\n",
        "                      allbadparts.append(bad)\n",
        "                      \n",
        "                      \n",
        "          if(len(allbadparts) > 0):\n",
        "              positions = utils.findpositions(allbadparts, sourcecode)\n",
        "\n",
        "              blocks = utils.getblocks(sourcecode, positions, step, fulllength)\n",
        "              \n",
        "              for b in blocks:\n",
        "\n",
        "                  allblocks.append(b)\n",
        "\n",
        "\n",
        "keys = []\n",
        "\n",
        "for i in range(len(allblocks)):\n",
        "  keys.append(i)\n",
        "random.shuffle(keys)\n",
        "\n",
        "\n",
        "cutoff = round(0.7 * len(keys)) \n",
        "cutoff2 = round(0.85 * len(keys)) \n",
        "\n",
        "keystrain = keys[:cutoff]\n",
        "keystrain = keystrain[:data_limit]\n",
        "keystest = keys[cutoff:cutoff2]\n",
        "keystest = keystest[:data_limit]\n",
        "keysfinaltest = keys[cutoff2:]\n",
        "keysfinaltest = keysfinaltest[:data_limit]\n",
        "\n",
        "print(\"cutoff \" + str(cutoff))\n",
        "print(\"cutoff2 \" + str(cutoff2))\n",
        "\n",
        "\n",
        "# with open('drive/MyDrive/SSS/data/dataset_keystrain', 'wb') as fp:\n",
        "#   pickle.dump(keystrain, fp)\n",
        "# with open('drive/MyDrive/SSS/data/dataset_keystest', 'wb') as fp:\n",
        "#   pickle.dump(keystest, fp)\n",
        "# with open('drive/MyDrive/SSS/data/dataset_keysfinaltest', 'wb') as fp:\n",
        "#   pickle.dump(keysfinaltest, fp)\n",
        "\n",
        "TrainX = []\n",
        "TrainY = []\n",
        "ValidateX = []\n",
        "ValidateY = []\n",
        "FinaltestX = []\n",
        "FinaltestY = []\n",
        "\n",
        "\n",
        "print(\"Creating training dataset... \")\n",
        "print(len(keystrain))\n",
        "\n",
        "for k in keystrain:\n",
        "  block = allblocks[k]    \n",
        "  code = block[0]\n",
        "  token = utils.getTokens(code) \n",
        "  vectorlist = []\n",
        "  for t in token: \n",
        "    if t != \" \" and word_vectors.key_to_index.get(t):\n",
        "      vector = model.wv.get_vector(t)\n",
        "      vectorlist.append(vector.tolist()) \n",
        "  TrainX.append(vectorlist) \n",
        "  TrainY.append(block[1]) \n",
        "\n",
        "print(\"Creating validation dataset...\")\n",
        "for k in keystest:\n",
        "  block = allblocks[k]\n",
        "  code = block[0]\n",
        "  token = utils.getTokens(code) \n",
        "  vectorlist = []\n",
        "  for t in token: \n",
        "    if t != \" \" and word_vectors.key_to_index.get(t):\n",
        "      vector = model.wv.get_vector(t)\n",
        "      vectorlist.append(vector.tolist()) \n",
        "  ValidateX.append(vectorlist) \n",
        "  ValidateY.append(block[1]) \n",
        "\n",
        "print(\"Creating finaltest dataset...\")\n",
        "for k in keysfinaltest:\n",
        "  block = allblocks[k]  \n",
        "  code = block[0]\n",
        "  token = utils.getTokens(code) \n",
        "  vectorlist = []\n",
        "  for t in token: \n",
        "    if t != \" \" and word_vectors.key_to_index.get(t):\n",
        "      vector = model.wv.get_vector(t)\n",
        "      vectorlist.append(vector.tolist()) \n",
        "  FinaltestX.append(vectorlist) \n",
        "  FinaltestY.append(block[1]) \n",
        "\n",
        "#for bert\n",
        "#print(\"Creating training dataset... \")\n",
        "#for k in keystrain:\n",
        "  #block = allblocks[k]    \n",
        "  #code = block[0]\n",
        "  #sentences = code.split('\\n')\n",
        "  #result = bert_embedding(sentences)\n",
        "  #for i in range(len(result)):\n",
        "    #token = result[i]\n",
        "    #tokens = token[1]\n",
        "  #TrainX.append(tokens) \n",
        "  #TrainY.append(block[1]) \n",
        "\n",
        "#print(\"Creating validation dataset...\")\n",
        "#for k in keystest:\n",
        "  #block = allblocks[k]\n",
        "  #code = block[0]\n",
        "  #sentences = code.split('\\n')\n",
        "  #result = bert_embedding(sentences)\n",
        "  #for i in range(len(result)):\n",
        "   #token = result[i]\n",
        "    #tokens = token[1]\n",
        "  #ValidateX.append(tokens) \n",
        "  #ValidateY.append(block[1]) \n",
        "\n",
        "#print(\"Creating finaltest dataset...\")\n",
        "#for k in keysfinaltest:\n",
        "  #block = allblocks[k]  \n",
        "  #code = block[0]\n",
        "  #sentences = code.split('\\n')\n",
        "  #result = bert_embedding(sentences)\n",
        "  #for i in range(len(result)):\n",
        "    #token = result[i]\n",
        "    #tokens = token[1]\n",
        "  #FinaltestX.append(tokens)  \n",
        "  #FinaltestY.append(block[1]) \n",
        "\n",
        "\n",
        "print(\"Train length: \" + str(len(TrainX)))\n",
        "print(\"Test length: \" + str(len(ValidateX)))\n",
        "print(\"Finaltesting length: \" + str(len(FinaltestX)))\n",
        "now = datetime.now() # current date and time\n",
        "nowformat = now.strftime(\"%H:%M\")\n",
        "print(\"time: \", nowformat)\n",
        "\n",
        "\n",
        "\n",
        "X_train =  numpy.array(TrainX)\n",
        "y_train =  numpy.array(TrainY)\n",
        "X_test =  numpy.array(ValidateX)\n",
        "y_test =  numpy.array(ValidateY)\n",
        "X_finaltest =  numpy.array(FinaltestX)\n",
        "y_finaltest =  numpy.array(FinaltestY)\n",
        "print(X_train.shape)\n",
        "for i in range(len(y_train)):\n",
        "  if y_train[i] == 0:\n",
        "    y_train[i] = 1\n",
        "  else:\n",
        "    y_train[i] = 0\n",
        "    \n",
        "for i in range(len(y_test)):\n",
        "  if y_test[i] == 0:\n",
        "    y_test[i] = 1\n",
        "  else:\n",
        "    y_test[i] = 0\n",
        "    \n",
        "for i in range(len(y_finaltest)):\n",
        "  if y_finaltest[i] == 0:\n",
        "    y_finaltest[i] = 1\n",
        "  else:\n",
        "    y_finaltest[i] = 0\n",
        "\n",
        "\n",
        "now = datetime.now() \n",
        "nowformat = now.strftime(\"%H:%M\")\n",
        "print(\"numpy array done. \", nowformat)\n",
        "\n",
        "print(str(len(X_train)) + \" samples in the training set.\")      \n",
        "print(str(len(X_test)) + \" samples in the validation set.\") \n",
        "print(str(len(X_finaltest)) + \" samples in the final test set.\")\n",
        "  \n",
        "csum = 0\n",
        "for a in y_train:\n",
        "  csum = csum+a\n",
        "print(\"percentage of vulnerable samples: \"  + str(int((csum / len(X_train)) * 10000)/100) + \"%\")\n",
        "  \n",
        "testvul = 0\n",
        "for y in y_test:\n",
        "  if y == 1:\n",
        "    testvul = testvul+1\n",
        "print(\"absolute amount of vulnerable samples in test set: \" + str(testvul))\n",
        "\n",
        "max_length = fulllength \n",
        "  \n",
        "\n",
        "#hyperparameters for the GRU model\n",
        "\n",
        "dropout = 0.5\n",
        "neurons = 100\n",
        "optimizer = \"adam\"\n",
        "epochs = 50\n",
        "batchsize = 128\n",
        "\n",
        "now = datetime.now() # current date and time\n",
        "nowformat = now.strftime(\"%H:%M\")\n",
        "print(\"Starting GRU: \", nowformat)\n",
        "\n",
        "\n",
        "print(\"Dropout: \" + str(dropout))\n",
        "print(\"Neurons: \" + str(neurons))\n",
        "print(\"Optimizer: \" + optimizer)\n",
        "print(\"Epochs: \" + str(epochs))\n",
        "print(\"Batch Size: \" + str(batchsize))\n",
        "print(\"max length: \" + str(max_length))\n",
        "\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=100, dtype='float32')\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=100, dtype='float32')\n",
        "X_finaltest = sequence.pad_sequences(X_finaltest, maxlen=100, dtype='float32')\n",
        "\n",
        "\n",
        "#creating the model  \n",
        "model = Sequential()\n",
        "#model.add(LSTM(neurons, dropout = dropout, recurrent_dropout = dropout)) #around 50 seems good\n",
        "model.add(GRU(neurons, dropout = dropout, recurrent_dropout = dropout)) #around 50 seems good\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss=utils.f1_loss, optimizer='adam', metrics=[utils.f1])\n",
        "\n",
        "now = datetime.now() \n",
        "nowformat = now.strftime(\"%H:%M\")\n",
        "print(\"Compiled LSTM: \", nowformat)\n",
        "\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced', classes=numpy.unique(y_train), y=y_train)\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "y_train = tf.cast(y_train, tf.float32)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batchsize, class_weight=class_weights)\n",
        "\n",
        "\n",
        "\n",
        "for dataset in [\"train\",\"test\",\"finaltest\"]:\n",
        "    print(\"Now predicting on \" + dataset + \" set (\" + str(dropout) + \" dropout)\")\n",
        "    \n",
        "    if dataset == \"train\":\n",
        "      yhat_classes = (model.predict(X_train) > 0.5).astype(\"float32\")\n",
        "      accuracy = accuracy_score(y_train, yhat_classes)\n",
        "      precision = precision_score(y_train, yhat_classes)\n",
        "      recall = recall_score(y_train, yhat_classes)\n",
        "      F1Score = f1_score(y_train, yhat_classes)\n",
        "      \n",
        "    if dataset == \"test\":\n",
        "      yhat_classes = (model.predict(X_test) > 0.5).astype(\"float32\")\n",
        "      accuracy = accuracy_score(y_test, yhat_classes)\n",
        "      precision = precision_score(y_test, yhat_classes)\n",
        "      recall = recall_score(y_test, yhat_classes)\n",
        "      F1Score = f1_score(y_test, yhat_classes)\n",
        "      \n",
        "      \n",
        "    if dataset == \"finaltest\":\n",
        "      yhat_classes = (model.predict(X_finaltest) > 0.5).astype(\"float32\")\n",
        "      accuracy = accuracy_score(y_finaltest, yhat_classes)\n",
        "      precision = precision_score(y_finaltest, yhat_classes)\n",
        "      recall = recall_score(y_finaltest, yhat_classes)\n",
        "      F1Score = f1_score(y_finaltest, yhat_classes)\n",
        "      \n",
        "    print(\"Accuracy: \" + str(accuracy))\n",
        "    print(\"Precision: \" + str(precision))\n",
        "    print(\"Recall: \" + str(recall))\n",
        "    print('F1 score: %f' % F1Score)\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "now = datetime.now() \n",
        "nowformat = now.strftime(\"%H:%M\")\n",
        "print(\"saving GRU model \"  \". \", nowformat)\n",
        "model.save(f'drive/MyDrive/SSS/GRU_model_{vulnerability_type}.h5')  \n",
        "print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaeZW2qd8fDm",
        "outputId": "d08b9644-fd1e-4351-c8f9-e2126cbc13b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install tqdm boto3 requests regex sentencepiece sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifEnZbuLDEYS",
        "outputId": "e8c5e061-402d-44eb-c123-eab96a48c871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.20.23-py3-none-any.whl (131 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "Collecting botocore<1.24.0,>=1.23.23\n",
            "  Downloading botocore-1.23.23-py3-none-any.whl (8.4 MB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.23->boto3) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.23->boto3) (1.15.0)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, sacremoses, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.20.23 botocore-1.23.23 jmespath-0.10.0 s3transfer-0.5.0 sacremoses-0.0.46 sentencepiece-0.1.96 urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim==4.1.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jxr4pqXMDF00",
        "outputId": "24aa960a-d09c-4b59-bf79-4751b5dc1b83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim==4.1.2\n",
            "  Downloading gensim-4.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.19.5)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "# Make sure using gensim 4.1.2\n",
        "gensim.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6Lb2thAfFc9V",
        "outputId": "e2ee893d-902c-426f-ec76-5a37b999622e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'4.1.2'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "jPga08m-oc7c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6XJniZm7knxx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}